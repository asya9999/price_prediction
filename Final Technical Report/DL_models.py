# -*- coding: utf-8 -*-
"""Копия блокнота "final_model.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BEkd8d3mua9C3a7Nu-Y5L4UCSUjTOK_w
"""

# Mount folder with Dataset
from google.colab import drive
drive.mount('/content/drive')

cd '/content/drive/MyDrive/Price_prediction_project'

import pandas as pd 

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split

import numpy as np

test = pd.read_csv("./test.tsv",sep='\t')
train = pd.read_csv("./train.tsv",sep='\t')

train.head()

# parameters

missing_value = "missing"
missing_cat_value = "missing"
categ_length = 3

#input to model
MAX_NAME_SEQ = 10
MAX_ITEM_DESC_SEQ = 30
MAX_CAT1_NAME_SEQ = 4
MAX_CAT2_NAME_SEQ = 4
MAX_CAT3_NAME_SEQ = 4

def handle_missing_inplace(input_dataset):
    def process_descr(text, t):
      if isinstance(text, str) and text!=t:
        return text
      else:
        return missing_value

    dataset = input_dataset.copy()
    dataset['category_name'].fillna(value = missing_value, inplace=True)
    dataset['brand_name'].fillna(value = missing_value, inplace=True)
    no_descr = "No description yet"
    dataset['item_description'] = dataset['item_description'].apply(lambda x: process_descr(x, no_descr))
    dataset['item_description'].fillna(value = missing_value, inplace=True)
    return dataset

def handle_cateogry(input_dataset):
  def split_category_name(text):
    try:
      new_text = text.split("/")
      while len(new_text) < categ_length:
        new_text.append(missing_cat_value)
      return new_text[:categ_length]
    except:
      return [missing_cat_value, missing_cat_value, missing_cat_value]

  dataset = input_dataset.copy()
  res = dataset['category_name'].apply(lambda x: split_category_name(x))
  dataset['cat_1'] = dataset['category_name'].apply(lambda x: split_category_name(x)[0])
  dataset['cat_2'] = dataset['category_name'].apply(lambda x: split_category_name(x)[1])
  dataset['cat_3'] = dataset['category_name'].apply(lambda x: split_category_name(x)[2])
  return dataset

def tokenize(input_dataset, tokenizer):
  # add padding and cropping( in case its longer)
  dataset = input_dataset.copy()
  dataset["seq_cat_1_name"] = tokenizer.texts_to_sequences(dataset.cat_1.str.lower())
  dataset["seq_cat_2_name"] = tokenizer.texts_to_sequences(dataset.cat_2.str.lower())
  dataset["seq_cat_3_name"] = tokenizer.texts_to_sequences(dataset.cat_3.str.lower())
  dataset["seq_item_description"] = tokenizer.texts_to_sequences(dataset.item_description.str.lower())
  dataset["seq_name"] = tokenizer.texts_to_sequences(dataset.name.str.lower())
  dataset["seq_brand_name"] = tokenizer.texts_to_sequences(dataset.brand_name.str.lower())
  return dataset


def make_tokenizer(input_dataset):
  raw_text = np.hstack([input_dataset.cat_1.str.lower(), 
                        input_dataset.cat_2.str.lower(), 
                        input_dataset.cat_3.str.lower(), 
                        input_dataset.item_description.str.lower(), 
                        input_dataset.name.str.lower(),
                        input_dataset.brand_name.str.lower()])
  tokenizer = Tokenizer()
  tokenizer.fit_on_texts(raw_text)
  return tokenizer

def make_y(data):
  return np.log1p(data)

def form_input_to_model(dataset): #, le_brand_name):
    X = {
        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ)
        ,'item_desc': pad_sequences(dataset.seq_item_description
                                    , maxlen=MAX_ITEM_DESC_SEQ)
        ,'cat_1': pad_sequences(dataset.seq_cat_1_name
                                        , maxlen=MAX_CAT1_NAME_SEQ)
        ,'cat_2': pad_sequences(dataset.seq_cat_2_name
                                        , maxlen=MAX_CAT2_NAME_SEQ)
        ,'cat_3': pad_sequences(dataset.seq_cat_3_name
                                        , maxlen=MAX_CAT3_NAME_SEQ)
        
        ,'brand_name': pad_sequences(dataset.seq_brand_name, maxlen=MAX_CAT3_NAME_SEQ)
        ,'item_condition': np.array(dataset.item_condition_id)
        ,'shipping': np.array(dataset.shipping)
    }
    return X

X_train, X_test, y_train, y_test = train_test_split(train, train['price'], test_size=0.2)

# Final version for prepare dataset
def prepare_train(dataset):
    new_train = handle_missing_inplace(dataset)
    new_train = handle_cateogry(new_train)

    result_tokenizer = make_tokenizer(new_train)
    new_train = tokenize(new_train, result_tokenizer)

    #lb_brand_name = LabelBinarizer()
    #lb_brand_name = lb_brand_name.fit(new_train.brand_name)
    
    X = form_input_to_model(new_train)#, lb_brand_name)

    return X, result_tokenizer#, lb_brand_name

X_train_new, result_tokenizer = prepare_train(X_train)

y_train_new = make_y(y_train)
y_train_new = np.array(y_train_new)

print(y_train_new)
print(X_train_new['shipping'].shape, X_train_new['shipping'])
print(X_train_new['item_condition'].shape, X_train_new['item_condition'])
print(X_train_new['brand_name'].shape)
print(X_train_new['brand_name'][4])
print(X_train_new['cat_3'].shape)
print(X_train_new['cat_3'][100])
print(X_train_new['item_desc'].shape)
print(X_train_new['item_desc'][100])
print(X_train_new['name'].shape)
print(X_train_new['name'][100])

def prepare_test(dataset):
    new_train = handle_missing_inplace(dataset)
    new_train = handle_cateogry(new_train)
    new_train = tokenize(new_train, result_tokenizer)
    X = form_input_to_model(new_train)#, lb_brand_name)

    return X

X_test_new = prepare_test(X_test)
y_test_new = make_y(y_test)
y_test_new = np.array(y_test_new)

from keras.layers import Input, Dropout, Dense, BatchNormalization, \
    Activation, concatenate, GRU, Embedding, Flatten
from keras.models import Model
from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping#, TensorBoard
from keras import backend as K
from keras import optimizers
from keras import initializers


def get_model():
    
    #Inputs
    name = Input(shape=[X_train_new["name"].shape[1]], name="name")
    item_desc = Input(shape=[X_train_new["item_desc"].shape[1]], name="item_desc")
    brand_name = Input(shape=[X_train_new["brand_name"].shape[1]], 
                          name="brand_name")
    cat_1 = Input(shape=[X_train_new["cat_1"].shape[1]], 
                          name="cat_1")
    cat_2 = Input(shape=[X_train_new["cat_2"].shape[1]], 
                          name="cat_2")
    cat_3 = Input(shape=[X_train_new["cat_3"].shape[1]], 
                          name="cat_3")
    item_condition = Input(shape=[1], name="item_condition")
    shipping = Input(shape=[1], name="shipping")
    
    #Embeddings layers
    emb_size = 60
    
    emb_name = Embedding(X_train_new["name"].max()+2, emb_size//3)(name)
    emb_item_desc = Embedding(X_train_new["item_desc"].max()+2, emb_size)(item_desc)
    emb_brand = Embedding(X_train_new["brand_name"].max()+2, 10)(brand_name)
    emb_cat_1 = Embedding(X_train_new["cat_1"].max()+2, 10)(cat_1)
    emb_cat_2 = Embedding(X_train_new["cat_2"].max()+2, 10)(cat_2)
    emb_cat_3 = Embedding(X_train_new["cat_3"].max()+2, 10)(cat_3)
    emb_item_condition = Embedding(X_train_new["item_condition"].max()+2, 5)(item_condition)
    
    rnn_layer1 = GRU(16) (emb_item_desc)
    rnn_layer2 = GRU(8) (emb_name)
    
    #main layer
    main_l = concatenate([
        Flatten() (emb_brand)
        , Flatten() (emb_cat_1)
        , Flatten() (emb_cat_2)
        , Flatten() (emb_cat_3)
        , Flatten() (emb_item_condition)
        , rnn_layer1
        , rnn_layer2
        , shipping
    ])
    main_l = Dropout(0.25)(Dense(128,activation='relu') (main_l))
    main_l = Dropout(0.1)(Dense(128,activation='relu') (main_l))
    
    #output
    output = Dense(1,activation="linear") (main_l)
    
    #model
    model = Model([name, item_desc, brand_name, cat_1, cat_2, cat_3, item_condition, shipping], output)
    #optimizer = optimizers.RMSprop()
    optimizer = optimizers.Adam()
    model.compile(loss="mse", optimizer=optimizer, metrics=['accuracy'])
    return model

model = get_model()
history = model.fit(X_train_new, y_train_new, batch_size=32, epochs=1, verbose=1, validation_data=(X_test_new, y_test_new))

score = model.evaluate(X_train_new, y_train_new, verbose=0)
print('Train loss:', score)
print('Train accuracy:', score)

score = model.evaluate(X_test_new, y_test_new, verbose=0)
print('Test loss:', score)
print('Test accuracy:', score)

import keras as ks
import pandas as pd
import numpy as np
import scipy
import tensorflow as tf

def mlp(X_train, y_train, X_test, y_test):
    
    input = ks.Input(shape=(X_train.shape[1],), dtype='float32', sparse=True)
    dense1 = ks.layers.Dense(256, activation='relu')(input)
    dense2 = ks.layers.Dense(64, activation='relu')(dense1)
    dense3 = ks.layers.Dense(64, activation='relu')(dense2)
    dense4 = ks.layers.Dense(32, activation='relu')(dense3)
    dense5 = ks.layers.Dense(1)(dense4)
    model = ks.Model(input, dense5)
    
    model.compile(loss='mean_squared_error', optimizer=ks.optimizers.Adam(lr=3e-3))
    model.fit(x=X_train, y=y_train, batch_size=32, epochs=3, verbose=1, validation_data=(X_test, y_test))
    
    return model


model_mlp = mlp(X_train_new, y_train_new, X_test_new, y_test_new)
model_mlp.summary()
pred1 = model_mlp.predict(X_test)[:, 0]